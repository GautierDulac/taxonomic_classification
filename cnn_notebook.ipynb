{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-functions\" data-toc-modified-id=\"Load-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load functions</a></span></li><li><span><a href=\"#Training-a-first-simple-NN\" data-toc-modified-id=\"Training-a-first-simple-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training a first simple NN</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-1D\" data-toc-modified-id=\"Looping-over-different-configuration-for-1D-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Looping over different configuration for 1D</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-2D\" data-toc-modified-id=\"Looping-over-different-configuration-for-2D-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Looping over different configuration for 2D</a></span></li><li><span><a href=\"#Looping-over-different-configurations-for-2D---ACM\" data-toc-modified-id=\"Looping-over-different-configurations-for-2D---ACM-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Looping over different configurations for 2D - ACM</a></span></li><li><span><a href=\"#ACTIVATION-MAP-Manually\" data-toc-modified-id=\"ACTIVATION-MAP-Manually-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>ACTIVATION MAP Manually</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tests-et-débug\" data-toc-modified-id=\"Tests-et-débug-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Tests et débug</a></span></li><li><span><a href=\"#From-ACM-python-file-adapted\" data-toc-modified-id=\"From-ACM-python-file-adapted-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>From ACM python file adapted</a></span></li></ul></li><li><span><a href=\"#Filter-visualisation-map\" data-toc-modified-id=\"Filter-visualisation-map-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Filter visualisation map</a></span></li><li><span><a href=\"#Get-model-structure-image\" data-toc-modified-id=\"Get-model-structure-image-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Get model structure image</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_model import classifier_GD_1, classifier_GD_2\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a first simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(selected_primer='V4', taxonomy_level=1)\n",
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin='DairyDB', primers_origin='DairyDB', selected_primer='V4', taxonomy_level=1)\n",
    "n_out_features = len(dict_class_to_id)\n",
    "\n",
    "out_channel_1 = 30  # 10\n",
    "out_channel_2 = 30  # 20\n",
    "kernel_size_1_W = 7  # 7\n",
    "kernel_size_2_W = 7  # 7\n",
    "ratio_fc_1 = 1 / 2  # 1 / 2\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "conv_class = classifier_GD_2(n_out_features)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "_, _, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - 2D',\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in 4 dimensions (k=1)',\n",
    "                     sequence_origin='DairyDB',\n",
    "                     primers_origin='DairyDB',\n",
    "                     taxonomy_level=1,\n",
    "                     selected_primer='V4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class classifier_GD_1(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        super(classifier_GD_1, self).__init__()\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1 = kernel_size_1\n",
    "        self.max_pool_stride_1 = max_pool_stride_1\n",
    "        self.max_pool_stride_2 = max_pool_stride_2\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1 = self.kernel_size_1\n",
    "        self.kernel_size_2 = self.kernel_size_1\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1 = max_size - self.kernel_size_1 + 1\n",
    "        self.L_out_max_pool_1 = int((self.L_out_conv_1 - self.kernel_size_1) // self.max_pool_stride_1) + 1\n",
    "        self.L_out_conv_2 = self.L_out_max_pool_1 - self.kernel_size_2 + 1\n",
    "        self.L_out_max_pool_2 = int((self.L_out_conv_2 - self.kernel_size_2) // self.max_pool_stride_2) + 1\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2 * self.ratio_fc_1)\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=4 ** k_mer, out_channels=self.out_channel_1,\n",
    "                               kernel_size=self.kernel_size_1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=self.kernel_size_2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm1d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.out_channel_2 * self.L_out_max_pool_2,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_1, stride=self.max_pool_stride_1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_2, stride=self.max_pool_stride_2)\n",
    "        x = x.view(-1, self.out_channel_2 * self.L_out_max_pool_2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1','max_pool_stride_1', 'max_pool_stride_2','ratio_fc_1','n_epochs','learning_rate','accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':50,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':100,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':150,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':200,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':250,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':300,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1 = parameter_config['kernel_size_1']\n",
    "    max_pool_stride_1 = parameter_config['max_pool_stride_1']\n",
    "    max_pool_stride_2 = parameter_config['max_pool_stride_2']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_1(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class classifier_GD_2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        super(classifier_GD_2, self).__init__()\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1_W = kernel_size_1_W\n",
    "        self.kernel_size_2_W = kernel_size_2_W\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # FIXED PARAMETERS\n",
    "        self.kernel_size_1_H = 4 ** k_mer\n",
    "        self.padding_conv_1_H = 0\n",
    "        self.padding_conv_1_W = 0\n",
    "        self.kernel_size_max_pool_1_H = 1\n",
    "        self.max_pool_stride_1_H = 1\n",
    "        self.max_pool_stride_1_W = 8\n",
    "        self.kernel_size_2_H = 1\n",
    "        self.padding_conv_2_H = 0\n",
    "        self.padding_conv_2_W = 0\n",
    "        self.kernel_size_max_pool_2_H = 1\n",
    "        self.max_pool_stride_2_H = 1\n",
    "        self.max_pool_stride_2_W = 8\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1_W = self.kernel_size_1_W  # 7\n",
    "        self.kernel_size_max_pool_2_W = self.kernel_size_2_W  # 7\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1_H = 4 ** k_mer - self.kernel_size_1_H + 2 * self.padding_conv_1_H + 1  # 1\n",
    "        self.L_out_conv_1_W = max_size - self.kernel_size_1_W + 2 * self.padding_conv_1_W + 1  # 294\n",
    "        self.L_out_max_pool_1_H = int((self.L_out_conv_1_H - self.kernel_size_max_pool_1_H) // self.max_pool_stride_1_H) + 1  # 1\n",
    "        self.L_out_max_pool_1_W = int((self.L_out_conv_1_W - self.kernel_size_max_pool_1_W) // self.max_pool_stride_1_W) + 1  # 36\n",
    "        self.L_out_conv_2_H = self.L_out_max_pool_1_H - self.kernel_size_2_H + 2 * self.padding_conv_2_H + 1  # 1\n",
    "        self.L_out_conv_2_W = self.L_out_max_pool_1_W - self.kernel_size_2_W + 2 * self.padding_conv_2_W + 1  # 30\n",
    "        self.L_out_max_pool_2_H = int((self.L_out_conv_2_H - self.kernel_size_max_pool_2_H) // self.max_pool_stride_2_H) + 1  # 1\n",
    "        self.L_out_max_pool_2_W = int((self.L_out_conv_2_W - self.kernel_size_max_pool_2_W) // self.max_pool_stride_2_W) + 1  # 4\n",
    "        self.L_in_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W)  # 80\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W * self.ratio_fc_1)  # 40\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.out_channel_1,\n",
    "                               kernel_size=(self.kernel_size_1_H, self.kernel_size_1_W),\n",
    "                               padding=(self.padding_conv_1_H, self.padding_conv_1_W))\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=(self.kernel_size_2_H, self.kernel_size_2_W),\n",
    "                               padding=(self.padding_conv_2_H, self.padding_conv_2_W))\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.L_in_fc_1,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 4 ** self.k_mer, self.max_size)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_1_H, self.kernel_size_max_pool_1_W),\n",
    "                         stride=(self.max_pool_stride_1_H, self.max_pool_stride_1_W))\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_2_H, self.kernel_size_max_pool_2_W),\n",
    "                         stride=(self.max_pool_stride_2_H, self.max_pool_stride_2_W))\n",
    "        x = x.view(-1, self.L_in_fc_1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','ratio_fc_1','n_epochs','learning_rate','accuracy', 'training_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':32, 'out_channel_2':64, 'kernel_size_1_W':8, 'kernel_size_2_W':8, 'max_pool_stride_1_W':8, 'max_pool_stride_2_W':8, 'ratio_fc_1':1/2, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1_W = parameter_config['kernel_size_1_W']\n",
    "    kernel_size_2_W = parameter_config['kernel_size_2_W']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_2(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configurations for 2D - ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_model import classifier_GD_2_ACM\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','n_epochs','learning_rate','accuracy','training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':5, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':254, 'out_channel_2':254, 'kernel_size_1_W':5, 'kernel_size_2_W':30, 'max_pool_stride_1_W':5, 'max_pool_stride_2_W':15, 'n_epochs':50, 'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':5, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':254, 'out_channel_2':254, 'kernel_size_1_W':5, 'kernel_size_2_W':30, 'max_pool_stride_1_W':5, 'max_pool_stride_2_W':30, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - V_ACM - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "\n",
    "    conv_class = classifier_GD_2_ACM(n_out_features, parameter_config)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     save_model=True,\n",
    "                     parameter_config=parameter_config,\n",
    "                     create_acm=True,\n",
    "                     acm_parameters=[X_test, y_test, 20],\n",
    "                     model_name='CNN - V_ACM - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIVATION MAP Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_acm import create_activation_map\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config = {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 300, 'out_channel_1': 64, 'out_channel_2': 128, 'kernel_size_1_W': 5, 'kernel_size_2_W': 5, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 5, 'n_epochs': 50, 'learning_rate': 0.001, 'accuracy': 0.6014234875444839, 'training_time': 975.4851434230804}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = parameter_config['vector_max_size']\n",
    "k_mer = parameter_config['k_mer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = 'D:\\\\0 - Boulot\\\\5 - X4\\\\16. Research Paper\\\\model_results\\\\CNN - V_ACM - 2D\\\\00008_analysis_V4_5_good\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_activation_map(X_test, y_test, dict_id_to_class, parameter_config, n=2, analysis_path=analysis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter visualisation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.loading_model_data import main_loading_model_data\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_preprocessing import get_homogenous_vector\n",
    "from models.cnn_acm import get_kernel_activation_map, create_one_activation_map_with_return, get_letters\n",
    "\n",
    "from models.cnn_model import classifier_GD_2_ACM\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config = {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 300, 'out_channel_1': 64, 'out_channel_2': 128, 'kernel_size_1_W': 5, 'kernel_size_2_W': 5, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 5, 'n_epochs': 50, 'learning_rate': 0.001, 'accuracy': 0.6014234875444839, 'training_time': 975.4851434230804}\n",
    "analysis_path = 'D:\\\\0 - Boulot\\\\5 - X4\\\\16. Research Paper\\\\model_results\\\\CNN - V_ACM - 2D\\\\00008_analysis_V4_5_good\\\\'\n",
    "model_path = analysis_path + 'model.pt'\n",
    "slash = '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = parameter_config['vector_max_size']\n",
    "k_mer = parameter_config['k_mer']\n",
    "\n",
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "\n",
    "n_out_features = len(dict_class_to_id)\n",
    "\n",
    "conv_class = classifier_GD_2_ACM(n_out_features=n_out_features,\n",
    "                                 parameter_config=parameter_config)\n",
    "\n",
    "model_path = analysis_path + 'model.pt'\n",
    "acm_path = analysis_path + 'ACM{}'.format(slash)\n",
    "\n",
    "conv_class.load_state_dict(torch.load(model_path))\n",
    "conv_class.eval()\n",
    "\n",
    "X_test_col = X_test.iloc[:, 1]\n",
    "y_test_col = y_test.iloc[:, 1]\n",
    "X_train_col = X_train.iloc[:, 1] \n",
    "y_train_col = y_train.iloc[:, 1] \n",
    "new_X_test = np.array([get_homogenous_vector(X_test_col[i], max_size).transpose() for i in range(len(X_test))])\n",
    "new_X_train = np.array([get_homogenous_vector(X_train_col[i], max_size).transpose() for i in range(len(X_train))])\n",
    "weight = conv_class.conv.conv1.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing kernel 63 over 64\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels = np.zeros((0,15350//5,3))\n",
    "gap = np.zeros((1,15350//5,3)) + 256\n",
    "all_acm = {}\n",
    "\n",
    "test_id = 3\n",
    "\n",
    "for i in range(weight.shape[0]):\n",
    "    print('Computing kernel {} over {}'.format(i, weight.shape[0]), end='\\r')\n",
    "    all_acm[get_letters(weight[i][0])] = get_kernel_activation_map(X_test_col, new_X_test, test_id, weight, i, max_size=300)\n",
    "\n",
    "\n",
    "all_kernels, dict_sample_pred, probs = create_one_activation_map_with_return(X_test_col, new_X_test, y_test_col, test_id, dict_id_to_class, parameter_config, analysis_path, max_size=300) \n",
    "\n",
    "all_kernels = np.concatenate((all_kernels, gap))\n",
    "all_kernels = np.concatenate((all_kernels, gap))\n",
    "\n",
    "for index, key in enumerate(sorted(all_acm.keys())) :  \n",
    "    all_kernels = np.concatenate((all_kernels, all_acm[key]))\n",
    "    if i != weight.shape[0] - 1:\n",
    "        all_kernels = np.concatenate((all_kernels, gap))\n",
    "\n",
    "cv2.imwrite('model_results{}00008_analysis_V4_5_good{}ACM_with_kernels{}real_{}_predicted_{}_proba_{}%.png'.\n",
    "            format(slash, slash, slash, dict_sample_pred['real_class'], dict_sample_pred['prediction'], int(probs*100)), \n",
    "            all_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model structure image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from models.cnn_model import classifier_GD_2_ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out_features = 44\n",
    "parameter_config = {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':128, 'out_channel_2':128, 'kernel_size_1_W':14, 'kernel_size_2_W':14, 'max_pool_stride_1_W':6, 'max_pool_stride_2_W':6, 'n_epochs':30, 'learning_rate':1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_class = classifier_GD_2_ACM(n_out_features, parameter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 4, 300).requires_grad_(True)\n",
    "y = conv_class(x)\n",
    "make_dot(y, params=dict(list(conv_class.named_parameters()) + [('x', x)])).render(\"attached\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
