{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-functions\" data-toc-modified-id=\"Load-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load functions</a></span></li><li><span><a href=\"#Training-a-first-simple-NN\" data-toc-modified-id=\"Training-a-first-simple-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training a first simple NN</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-1D\" data-toc-modified-id=\"Looping-over-different-configuration-for-1D-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Looping over different configuration for 1D</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-2D\" data-toc-modified-id=\"Looping-over-different-configuration-for-2D-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Looping over different configuration for 2D</a></span></li><li><span><a href=\"#Looping-over-different-configurations-for-2D---ACM\" data-toc-modified-id=\"Looping-over-different-configurations-for-2D---ACM-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Looping over different configurations for 2D - ACM</a></span></li><li><span><a href=\"#ACTIVATION-MAP-Manually\" data-toc-modified-id=\"ACTIVATION-MAP-Manually-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>ACTIVATION MAP Manually</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tests-et-débug\" data-toc-modified-id=\"Tests-et-débug-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Tests et débug</a></span></li><li><span><a href=\"#From-ACM-python-file-adapted\" data-toc-modified-id=\"From-ACM-python-file-adapted-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>From ACM python file adapted</a></span></li></ul></li><li><span><a href=\"#Filter-visualisation-map\" data-toc-modified-id=\"Filter-visualisation-map-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Filter visualisation map</a></span></li><li><span><a href=\"#Get-model-structure-image\" data-toc-modified-id=\"Get-model-structure-image-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Get model structure image</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_model import classifier_GD_1, classifier_GD_2\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a first simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(selected_primer='V4', taxonomy_level=1)\n",
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin='DairyDB', primers_origin='DairyDB', selected_primer='V4', taxonomy_level=1)\n",
    "n_out_features = len(dict_class_to_id)\n",
    "\n",
    "out_channel_1 = 30  # 10\n",
    "out_channel_2 = 30  # 20\n",
    "kernel_size_1_W = 7  # 7\n",
    "kernel_size_2_W = 7  # 7\n",
    "ratio_fc_1 = 1 / 2  # 1 / 2\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "conv_class = classifier_GD_2(n_out_features)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "_, _, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - 2D',\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in 4 dimensions (k=1)',\n",
    "                     sequence_origin='DairyDB',\n",
    "                     primers_origin='DairyDB',\n",
    "                     taxonomy_level=1,\n",
    "                     selected_primer='V4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class classifier_GD_1(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        super(classifier_GD_1, self).__init__()\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1 = kernel_size_1\n",
    "        self.max_pool_stride_1 = max_pool_stride_1\n",
    "        self.max_pool_stride_2 = max_pool_stride_2\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1 = self.kernel_size_1\n",
    "        self.kernel_size_2 = self.kernel_size_1\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1 = max_size - self.kernel_size_1 + 1\n",
    "        self.L_out_max_pool_1 = int((self.L_out_conv_1 - self.kernel_size_1) // self.max_pool_stride_1) + 1\n",
    "        self.L_out_conv_2 = self.L_out_max_pool_1 - self.kernel_size_2 + 1\n",
    "        self.L_out_max_pool_2 = int((self.L_out_conv_2 - self.kernel_size_2) // self.max_pool_stride_2) + 1\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2 * self.ratio_fc_1)\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=4 ** k_mer, out_channels=self.out_channel_1,\n",
    "                               kernel_size=self.kernel_size_1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=self.kernel_size_2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm1d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.out_channel_2 * self.L_out_max_pool_2,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_1, stride=self.max_pool_stride_1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_2, stride=self.max_pool_stride_2)\n",
    "        x = x.view(-1, self.out_channel_2 * self.L_out_max_pool_2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1','max_pool_stride_1', 'max_pool_stride_2','ratio_fc_1','n_epochs','learning_rate','accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':50,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':100,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':150,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':200,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':250,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':300,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1 = parameter_config['kernel_size_1']\n",
    "    max_pool_stride_1 = parameter_config['max_pool_stride_1']\n",
    "    max_pool_stride_2 = parameter_config['max_pool_stride_2']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_1(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class classifier_GD_2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        super(classifier_GD_2, self).__init__()\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1_W = kernel_size_1_W\n",
    "        self.kernel_size_2_W = kernel_size_2_W\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # FIXED PARAMETERS\n",
    "        self.kernel_size_1_H = 4 ** k_mer\n",
    "        self.padding_conv_1_H = 0\n",
    "        self.padding_conv_1_W = 0\n",
    "        self.kernel_size_max_pool_1_H = 1\n",
    "        self.max_pool_stride_1_H = 1\n",
    "        self.max_pool_stride_1_W = 8\n",
    "        self.kernel_size_2_H = 1\n",
    "        self.padding_conv_2_H = 0\n",
    "        self.padding_conv_2_W = 0\n",
    "        self.kernel_size_max_pool_2_H = 1\n",
    "        self.max_pool_stride_2_H = 1\n",
    "        self.max_pool_stride_2_W = 8\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1_W = self.kernel_size_1_W  # 7\n",
    "        self.kernel_size_max_pool_2_W = self.kernel_size_2_W  # 7\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1_H = 4 ** k_mer - self.kernel_size_1_H + 2 * self.padding_conv_1_H + 1  # 1\n",
    "        self.L_out_conv_1_W = max_size - self.kernel_size_1_W + 2 * self.padding_conv_1_W + 1  # 294\n",
    "        self.L_out_max_pool_1_H = int((self.L_out_conv_1_H - self.kernel_size_max_pool_1_H) // self.max_pool_stride_1_H) + 1  # 1\n",
    "        self.L_out_max_pool_1_W = int((self.L_out_conv_1_W - self.kernel_size_max_pool_1_W) // self.max_pool_stride_1_W) + 1  # 36\n",
    "        self.L_out_conv_2_H = self.L_out_max_pool_1_H - self.kernel_size_2_H + 2 * self.padding_conv_2_H + 1  # 1\n",
    "        self.L_out_conv_2_W = self.L_out_max_pool_1_W - self.kernel_size_2_W + 2 * self.padding_conv_2_W + 1  # 30\n",
    "        self.L_out_max_pool_2_H = int((self.L_out_conv_2_H - self.kernel_size_max_pool_2_H) // self.max_pool_stride_2_H) + 1  # 1\n",
    "        self.L_out_max_pool_2_W = int((self.L_out_conv_2_W - self.kernel_size_max_pool_2_W) // self.max_pool_stride_2_W) + 1  # 4\n",
    "        self.L_in_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W)  # 80\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W * self.ratio_fc_1)  # 40\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.out_channel_1,\n",
    "                               kernel_size=(self.kernel_size_1_H, self.kernel_size_1_W),\n",
    "                               padding=(self.padding_conv_1_H, self.padding_conv_1_W))\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=(self.kernel_size_2_H, self.kernel_size_2_W),\n",
    "                               padding=(self.padding_conv_2_H, self.padding_conv_2_W))\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.L_in_fc_1,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 4 ** self.k_mer, self.max_size)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_1_H, self.kernel_size_max_pool_1_W),\n",
    "                         stride=(self.max_pool_stride_1_H, self.max_pool_stride_1_W))\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_2_H, self.kernel_size_max_pool_2_W),\n",
    "                         stride=(self.max_pool_stride_2_H, self.max_pool_stride_2_W))\n",
    "        x = x.view(-1, self.L_in_fc_1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','ratio_fc_1','n_epochs','learning_rate','accuracy', 'training_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':32, 'out_channel_2':64, 'kernel_size_1_W':8, 'kernel_size_2_W':8, 'max_pool_stride_1_W':8, 'max_pool_stride_2_W':8, 'ratio_fc_1':1/2, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1_W = parameter_config['kernel_size_1_W']\n",
    "    kernel_size_2_W = parameter_config['kernel_size_2_W']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_2(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configurations for 2D - ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/dataframe/utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from models.cnn_model import classifier_GD_2_ACM\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn, slash, taxonomy_levels, folder_paths\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','n_epochs','learning_rate','accuracy','training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':5, 'dimension':2, 'k_mer':1, 'vector_max_size':270, 'out_channel_1':300, 'out_channel_2':300, 'kernel_size_1_W':5, 'kernel_size_2_W':15, 'max_pool_stride_1_W':5, 'max_pool_stride_2_W':15, 'n_epochs':50, 'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':5, 'dimension':2, 'k_mer':1, 'vector_max_size':270, 'out_channel_1':400, 'out_channel_2':400, 'kernel_size_1_W':5, 'kernel_size_2_W':15, 'max_pool_stride_1_W':5, 'max_pool_stride_2_W':15, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the following parameter configuration: \n",
      " {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 270, 'out_channel_1': 300, 'out_channel_2': 300, 'kernel_size_1_W': 5, 'kernel_size_2_W': 15, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 15, 'n_epochs': 50, 'learning_rate': 0.001}\n",
      "Test - Loss: 0.1182 Acc: 0.0011\n",
      "Epoch 1 over 50\n",
      "Train - Loss: 0.0985 Acc: 0.1497\n",
      "Test - Loss: 0.0865 Acc: 0.2346\n",
      "Epoch 2 over 50\n",
      "Train - Loss: 0.0855 Acc: 0.1756\n",
      "Test - Loss: 0.0828 Acc: 0.2261\n",
      "Epoch 3 over 50\n",
      "Train - Loss: 0.0706 Acc: 0.2717\n",
      "Test - Loss: 0.0747 Acc: 0.2930\n",
      "Epoch 4 over 50\n",
      "Train - Loss: 0.0588 Acc: 0.3531\n",
      "Test - Loss: 0.0669 Acc: 0.3657\n",
      "Epoch 5 over 50\n",
      "Train - Loss: 0.0483 Acc: 0.4326\n",
      "Test - Loss: 0.0606 Acc: 0.4204\n",
      "Epoch 6 over 50\n",
      "Train - Loss: 0.0399 Acc: 0.4976\n",
      "Test - Loss: 0.0566 Acc: 0.4549\n",
      "Epoch 7 over 50\n",
      "Train - Loss: 0.0326 Acc: 0.5562\n",
      "Test - Loss: 0.0554 Acc: 0.4846\n",
      "Epoch 8 over 50\n",
      "Train - Loss: 0.0274 Acc: 0.6087\n",
      "Test - Loss: 0.0531 Acc: 0.4878\n",
      "Epoch 9 over 50\n",
      "Train - Loss: 0.0230 Acc: 0.6443\n",
      "Test - Loss: 0.0512 Acc: 0.5122\n",
      "Epoch 10 over 50\n",
      "Train - Loss: 0.0196 Acc: 0.6830\n",
      "Test - Loss: 0.0537 Acc: 0.5340\n",
      "Epoch 11 over 50\n",
      "Train - Loss: 0.0172 Acc: 0.7048\n",
      "Test - Loss: 0.0535 Acc: 0.5414\n",
      "Epoch 12 over 50\n",
      "Train - Loss: 0.0150 Acc: 0.7417\n",
      "Test - Loss: 0.0522 Acc: 0.5626\n",
      "Epoch 13 over 50\n",
      "Train - Loss: 0.0133 Acc: 0.7609\n",
      "Test - Loss: 0.0515 Acc: 0.5732\n",
      "Epoch 14 over 50\n",
      "Train - Loss: 0.0118 Acc: 0.7831\n",
      "Test - Loss: 0.0524 Acc: 0.5770\n",
      "Epoch 15 over 50\n",
      "Train - Loss: 0.0106 Acc: 0.8048\n",
      "Test - Loss: 0.0541 Acc: 0.5791\n",
      "Epoch 16 over 50\n",
      "Train - Loss: 0.0098 Acc: 0.8171\n",
      "Test - Loss: 0.0549 Acc: 0.5860\n",
      "Epoch 17 over 50\n",
      "Train - Loss: 0.0091 Acc: 0.8273\n",
      "Test - Loss: 0.0538 Acc: 0.5743\n",
      "Epoch 18 over 50\n",
      "Train - Loss: 0.0082 Acc: 0.8436\n",
      "Test - Loss: 0.0535 Acc: 0.5892\n",
      "Epoch 19 over 50\n",
      "Train - Loss: 0.0078 Acc: 0.8465\n",
      "Test - Loss: 0.0561 Acc: 0.5977\n",
      "Epoch 20 over 50\n",
      "Train - Loss: 0.0072 Acc: 0.8583\n",
      "Test - Loss: 0.0559 Acc: 0.5987\n",
      "Epoch 21 over 50\n",
      "Train - Loss: 0.0064 Acc: 0.8691\n",
      "Test - Loss: 0.0536 Acc: 0.6019\n",
      "Epoch 22 over 50\n",
      "Train - Loss: 0.0065 Acc: 0.8697\n",
      "Test - Loss: 0.0580 Acc: 0.5886\n",
      "Epoch 23 over 50\n",
      "Train - Loss: 0.0061 Acc: 0.8788\n",
      "Test - Loss: 0.0552 Acc: 0.6051\n",
      "Epoch 24 over 50\n",
      "Train - Loss: 0.0055 Acc: 0.8901\n",
      "Test - Loss: 0.0596 Acc: 0.5849\n",
      "Epoch 25 over 50\n",
      "Train - Loss: 0.0055 Acc: 0.8902\n",
      "Test - Loss: 0.0571 Acc: 0.6019\n",
      "Epoch 26 over 50\n",
      "Train - Loss: 0.0054 Acc: 0.8914\n",
      "Test - Loss: 0.0578 Acc: 0.5865\n",
      "Epoch 27 over 50\n",
      "Train - Loss: 0.0050 Acc: 0.8976\n",
      "Test - Loss: 0.0625 Acc: 0.5987\n",
      "Epoch 28 over 50\n",
      "Train - Loss: 0.0047 Acc: 0.9024\n",
      "Test - Loss: 0.0606 Acc: 0.5977\n",
      "Epoch 29 over 50\n",
      "Train - Loss: 0.0047 Acc: 0.9020\n",
      "Test - Loss: 0.0596 Acc: 0.6072\n",
      "Epoch 30 over 50\n",
      "Train - Loss: 0.0043 Acc: 0.9144\n",
      "Test - Loss: 0.0612 Acc: 0.6093\n",
      "Epoch 31 over 50\n",
      "Train - Loss: 0.0044 Acc: 0.9097\n",
      "Test - Loss: 0.0625 Acc: 0.6083\n",
      "Epoch 32 over 50\n",
      "Train - Loss: 0.0041 Acc: 0.9164\n",
      "Test - Loss: 0.0639 Acc: 0.5945\n",
      "Epoch 33 over 50\n",
      "Train - Loss: 0.0039 Acc: 0.9186\n",
      "Test - Loss: 0.0600 Acc: 0.6062\n",
      "Epoch 34 over 50\n",
      "Train - Loss: 0.0038 Acc: 0.9228\n",
      "Test - Loss: 0.0614 Acc: 0.6046\n",
      "Epoch 35 over 50\n",
      "Train - Loss: 0.0038 Acc: 0.9213\n",
      "Test - Loss: 0.0602 Acc: 0.6178\n",
      "Epoch 36 over 50\n",
      "Train - Loss: 0.0035 Acc: 0.9299\n",
      "Test - Loss: 0.0610 Acc: 0.6067\n",
      "Epoch 37 over 50\n",
      "Train - Loss: 0.0038 Acc: 0.9220\n",
      "Test - Loss: 0.0660 Acc: 0.5971\n",
      "Epoch 38 over 50\n",
      "Train - Loss: 0.0035 Acc: 0.9287\n",
      "Test - Loss: 0.0616 Acc: 0.6019\n",
      "Epoch 39 over 50\n",
      "Train - Loss: 0.0034 Acc: 0.9269\n",
      "Test - Loss: 0.0634 Acc: 0.6093\n",
      "Epoch 40 over 50\n",
      "Train - Loss: 0.0032 Acc: 0.9312\n",
      "Test - Loss: 0.0622 Acc: 0.6162\n",
      "Epoch 41 over 50\n",
      "Train - Loss: 0.0035 Acc: 0.9279\n",
      "Test - Loss: 0.0640 Acc: 0.6056\n",
      "Epoch 42 over 50\n",
      "Train - Loss: 0.0032 Acc: 0.9319\n",
      "Test - Loss: 0.0638 Acc: 0.6178\n",
      "Epoch 43 over 50\n",
      "Train - Loss: 0.0031 Acc: 0.9363\n",
      "Test - Loss: 0.0666 Acc: 0.6120\n",
      "Epoch 44 over 50\n",
      "Train - Loss: 0.0031 Acc: 0.9366\n",
      "Test - Loss: 0.0644 Acc: 0.6030\n",
      "Epoch 45 over 50\n",
      "Train - Loss: 0.0028 Acc: 0.9428\n",
      "Test - Loss: 0.0631 Acc: 0.6120\n",
      "Epoch 46 over 50\n",
      "Train - Loss: 0.0029 Acc: 0.9395\n",
      "Test - Loss: 0.0657 Acc: 0.6120\n",
      "Epoch 47 over 50\n",
      "Train - Loss: 0.0030 Acc: 0.9393\n",
      "Test - Loss: 0.0666 Acc: 0.5998\n",
      "Epoch 48 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9430\n",
      "Test - Loss: 0.0688 Acc: 0.6136\n",
      "Epoch 49 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9450\n",
      "Test - Loss: 0.0675 Acc: 0.6173\n",
      "Epoch 50 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9442\n",
      "Test - Loss: 0.0637 Acc: 0.6221\n",
      "Test - Loss: 0.0630 Acc: 0.6072\n",
      "Working with the following parameter configuration: \n",
      " {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 270, 'out_channel_1': 400, 'out_channel_2': 400, 'kernel_size_1_W': 5, 'kernel_size_2_W': 15, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 15, 'n_epochs': 50, 'learning_rate': 0.001}\n",
      "Test - Loss: 0.1186 Acc: 0.0000\n",
      "Epoch 1 over 50\n",
      "Train - Loss: 0.0983 Acc: 0.1662\n",
      "Test - Loss: 0.0850 Acc: 0.2505\n",
      "Epoch 2 over 50\n",
      "Train - Loss: 0.0844 Acc: 0.1815\n",
      "Test - Loss: 0.0828 Acc: 0.2378\n",
      "Epoch 3 over 50\n",
      "Train - Loss: 0.0697 Acc: 0.2759\n",
      "Test - Loss: 0.0747 Acc: 0.2967\n",
      "Epoch 4 over 50\n",
      "Train - Loss: 0.0581 Acc: 0.3518\n",
      "Test - Loss: 0.0650 Acc: 0.3710\n",
      "Epoch 5 over 50\n",
      "Train - Loss: 0.0477 Acc: 0.4316\n",
      "Test - Loss: 0.0607 Acc: 0.4045\n",
      "Epoch 6 over 50\n",
      "Train - Loss: 0.0390 Acc: 0.5037\n",
      "Test - Loss: 0.0566 Acc: 0.4613\n",
      "Epoch 7 over 50\n",
      "Train - Loss: 0.0325 Acc: 0.5578\n",
      "Test - Loss: 0.0557 Acc: 0.4756\n",
      "Epoch 8 over 50\n",
      "Train - Loss: 0.0272 Acc: 0.5991\n",
      "Test - Loss: 0.0537 Acc: 0.5111\n",
      "Epoch 9 over 50\n",
      "Train - Loss: 0.0229 Acc: 0.6435\n",
      "Test - Loss: 0.0518 Acc: 0.5297\n",
      "Epoch 10 over 50\n",
      "Train - Loss: 0.0194 Acc: 0.6833\n",
      "Test - Loss: 0.0524 Acc: 0.5472\n",
      "Epoch 11 over 50\n",
      "Train - Loss: 0.0169 Acc: 0.7123\n",
      "Test - Loss: 0.0522 Acc: 0.5547\n",
      "Epoch 12 over 50\n",
      "Train - Loss: 0.0149 Acc: 0.7398\n",
      "Test - Loss: 0.0535 Acc: 0.5610\n",
      "Epoch 13 over 50\n",
      "Train - Loss: 0.0136 Acc: 0.7576\n",
      "Test - Loss: 0.0508 Acc: 0.5759\n",
      "Epoch 14 over 50\n",
      "Train - Loss: 0.0114 Acc: 0.7879\n",
      "Test - Loss: 0.0535 Acc: 0.5663\n",
      "Epoch 15 over 50\n",
      "Train - Loss: 0.0106 Acc: 0.7992\n",
      "Test - Loss: 0.0572 Acc: 0.5695\n",
      "Epoch 16 over 50\n",
      "Train - Loss: 0.0097 Acc: 0.8151\n",
      "Test - Loss: 0.0541 Acc: 0.5717\n",
      "Epoch 17 over 50\n",
      "Train - Loss: 0.0088 Acc: 0.8343\n",
      "Test - Loss: 0.0571 Acc: 0.5801\n",
      "Epoch 18 over 50\n",
      "Train - Loss: 0.0083 Acc: 0.8372\n",
      "Test - Loss: 0.0555 Acc: 0.5770\n",
      "Epoch 19 over 50\n",
      "Train - Loss: 0.0078 Acc: 0.8529\n",
      "Test - Loss: 0.0564 Acc: 0.5860\n",
      "Epoch 20 over 50\n",
      "Train - Loss: 0.0073 Acc: 0.8533\n",
      "Test - Loss: 0.0559 Acc: 0.5934\n",
      "Epoch 21 over 50\n",
      "Train - Loss: 0.0068 Acc: 0.8649\n",
      "Test - Loss: 0.0552 Acc: 0.6024\n",
      "Epoch 22 over 50\n",
      "Train - Loss: 0.0066 Acc: 0.8737\n",
      "Test - Loss: 0.0569 Acc: 0.5971\n",
      "Epoch 23 over 50\n",
      "Train - Loss: 0.0060 Acc: 0.8788\n",
      "Test - Loss: 0.0579 Acc: 0.6024\n",
      "Epoch 24 over 50\n",
      "Train - Loss: 0.0056 Acc: 0.8894\n",
      "Test - Loss: 0.0573 Acc: 0.6099\n",
      "Epoch 25 over 50\n",
      "Train - Loss: 0.0056 Acc: 0.8912\n",
      "Test - Loss: 0.0595 Acc: 0.6051\n",
      "Epoch 26 over 50\n",
      "Train - Loss: 0.0050 Acc: 0.8965\n",
      "Test - Loss: 0.0572 Acc: 0.6093\n",
      "Epoch 27 over 50\n",
      "Train - Loss: 0.0048 Acc: 0.9024\n",
      "Test - Loss: 0.0612 Acc: 0.5939\n",
      "Epoch 28 over 50\n",
      "Train - Loss: 0.0047 Acc: 0.9007\n",
      "Test - Loss: 0.0612 Acc: 0.6109\n",
      "Epoch 29 over 50\n",
      "Train - Loss: 0.0042 Acc: 0.9131\n",
      "Test - Loss: 0.0649 Acc: 0.5998\n",
      "Epoch 30 over 50\n",
      "Train - Loss: 0.0044 Acc: 0.9103\n",
      "Test - Loss: 0.0610 Acc: 0.5961\n",
      "Epoch 31 over 50\n",
      "Train - Loss: 0.0044 Acc: 0.9110\n",
      "Test - Loss: 0.0614 Acc: 0.6115\n",
      "Epoch 32 over 50\n",
      "Train - Loss: 0.0040 Acc: 0.9173\n",
      "Test - Loss: 0.0633 Acc: 0.6173\n",
      "Epoch 33 over 50\n",
      "Train - Loss: 0.0037 Acc: 0.9229\n",
      "Test - Loss: 0.0649 Acc: 0.6178\n",
      "Epoch 34 over 50\n",
      "Train - Loss: 0.0037 Acc: 0.9241\n",
      "Test - Loss: 0.0617 Acc: 0.6019\n",
      "Epoch 35 over 50\n",
      "Train - Loss: 0.0040 Acc: 0.9185\n",
      "Test - Loss: 0.0604 Acc: 0.6040\n",
      "Epoch 36 over 50\n",
      "Train - Loss: 0.0035 Acc: 0.9271\n",
      "Test - Loss: 0.0629 Acc: 0.6125\n",
      "Epoch 37 over 50\n",
      "Train - Loss: 0.0036 Acc: 0.9279\n",
      "Test - Loss: 0.0657 Acc: 0.6062\n",
      "Epoch 38 over 50\n",
      "Train - Loss: 0.0035 Acc: 0.9290\n",
      "Test - Loss: 0.0654 Acc: 0.6093\n",
      "Epoch 39 over 50\n",
      "Train - Loss: 0.0033 Acc: 0.9312\n",
      "Test - Loss: 0.0627 Acc: 0.6051\n",
      "Epoch 40 over 50\n",
      "Train - Loss: 0.0034 Acc: 0.9275\n",
      "Test - Loss: 0.0657 Acc: 0.6035\n",
      "Epoch 41 over 50\n",
      "Train - Loss: 0.0031 Acc: 0.9364\n",
      "Test - Loss: 0.0667 Acc: 0.6056\n",
      "Epoch 42 over 50\n",
      "Train - Loss: 0.0029 Acc: 0.9389\n",
      "Test - Loss: 0.0687 Acc: 0.6003\n",
      "Epoch 43 over 50\n",
      "Train - Loss: 0.0032 Acc: 0.9333\n",
      "Test - Loss: 0.0649 Acc: 0.6109\n",
      "Epoch 44 over 50\n",
      "Train - Loss: 0.0031 Acc: 0.9359\n",
      "Test - Loss: 0.0666 Acc: 0.6088\n",
      "Epoch 45 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9418\n",
      "Test - Loss: 0.0669 Acc: 0.6200\n",
      "Epoch 46 over 50\n",
      "Train - Loss: 0.0026 Acc: 0.9468\n",
      "Test - Loss: 0.0683 Acc: 0.6109\n",
      "Epoch 47 over 50\n",
      "Train - Loss: 0.0028 Acc: 0.9384\n",
      "Test - Loss: 0.0685 Acc: 0.6003\n",
      "Epoch 48 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9433\n",
      "Test - Loss: 0.0663 Acc: 0.6051\n",
      "Epoch 49 over 50\n",
      "Train - Loss: 0.0026 Acc: 0.9488\n",
      "Test - Loss: 0.0684 Acc: 0.6173\n",
      "Epoch 50 over 50\n",
      "Train - Loss: 0.0027 Acc: 0.9428\n",
      "Test - Loss: 0.0651 Acc: 0.6157\n",
      "Test - Loss: 0.0665 Acc: 0.6030\n"
     ]
    }
   ],
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results{}models{}CNN {}D - V_ACM - CNN({}) - accuracies.csv'.format(slash, slash,\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "\n",
    "    conv_class = classifier_GD_2_ACM(n_out_features, parameter_config)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     save_model=True,\n",
    "                     parameter_config=parameter_config,\n",
    "                     create_acm=True,\n",
    "                     acm_parameters=[X_test, y_test, 20],\n",
    "                     model_name='CNN - V_ACM - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIVATION MAP Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_acm import create_activation_map\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config = {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 300, 'out_channel_1': 64, 'out_channel_2': 128, 'kernel_size_1_W': 5, 'kernel_size_2_W': 5, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 5, 'n_epochs': 50, 'learning_rate': 0.001, 'accuracy': 0.6014234875444839, 'training_time': 975.4851434230804}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = parameter_config['vector_max_size']\n",
    "k_mer = parameter_config['k_mer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = 'D:\\\\0 - Boulot\\\\5 - X4\\\\16. Research Paper\\\\model_results\\\\CNN - V_ACM - 2D\\\\00008_analysis_V4_5_good\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_activation_map(X_test, y_test, dict_id_to_class, parameter_config, n=2, analysis_path=analysis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter visualisation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.loading_model_data import main_loading_model_data\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_preprocessing import get_homogenous_vector\n",
    "from models.cnn_acm import get_kernel_activation_map, create_one_activation_map_with_return, get_letters\n",
    "\n",
    "from models.cnn_model import classifier_GD_2_ACM\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config = {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 5, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 300, 'out_channel_1': 64, 'out_channel_2': 128, 'kernel_size_1_W': 5, 'kernel_size_2_W': 5, 'max_pool_stride_1_W': 5, 'max_pool_stride_2_W': 5, 'n_epochs': 50, 'learning_rate': 0.001, 'accuracy': 0.6014234875444839, 'training_time': 975.4851434230804}\n",
    "analysis_path = 'D:\\\\0 - Boulot\\\\5 - X4\\\\16. Research Paper\\\\model_results\\\\CNN - V_ACM - 2D\\\\00008_analysis_V4_5_good\\\\'\n",
    "model_path = analysis_path + 'model.pt'\n",
    "slash = '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = parameter_config['vector_max_size']\n",
    "k_mer = parameter_config['k_mer']\n",
    "\n",
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "\n",
    "n_out_features = len(dict_class_to_id)\n",
    "\n",
    "conv_class = classifier_GD_2_ACM(n_out_features=n_out_features,\n",
    "                                 parameter_config=parameter_config)\n",
    "\n",
    "model_path = analysis_path + 'model.pt'\n",
    "acm_path = analysis_path + 'ACM{}'.format(slash)\n",
    "\n",
    "conv_class.load_state_dict(torch.load(model_path))\n",
    "conv_class.eval()\n",
    "\n",
    "X_test_col = X_test.iloc[:, 1]\n",
    "y_test_col = y_test.iloc[:, 1]\n",
    "X_train_col = X_train.iloc[:, 1] \n",
    "y_train_col = y_train.iloc[:, 1] \n",
    "new_X_test = np.array([get_homogenous_vector(X_test_col[i], max_size).transpose() for i in range(len(X_test))])\n",
    "new_X_train = np.array([get_homogenous_vector(X_train_col[i], max_size).transpose() for i in range(len(X_train))])\n",
    "weight = conv_class.conv.conv1.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = np.zeros((0,15350//5,3))\n",
    "gap = np.zeros((1,15350//5,3)) + 256\n",
    "all_acm = {}\n",
    "\n",
    "test_id = 3\n",
    "\n",
    "for i in range(weight.shape[0]):\n",
    "    print('Computing kernel {} over {}'.format(i, weight.shape[0]), end='\\r')\n",
    "    all_acm[get_letters(weight[i][0])] = get_kernel_activation_map(X_test_col, new_X_test, test_id, weight, i, max_size=300)\n",
    "\n",
    "\n",
    "all_kernels, dict_sample_pred, probs = create_one_activation_map_with_return(X_test_col, new_X_test, y_test_col, test_id, dict_id_to_class, parameter_config, analysis_path, max_size=300) \n",
    "\n",
    "all_kernels = np.concatenate((all_kernels, gap))\n",
    "all_kernels = np.concatenate((all_kernels, gap))\n",
    "\n",
    "for index, key in enumerate(sorted(all_acm.keys())) :  \n",
    "    all_kernels = np.concatenate((all_kernels, all_acm[key]))\n",
    "    if i != weight.shape[0] - 1:\n",
    "        all_kernels = np.concatenate((all_kernels, gap))\n",
    "\n",
    "cv2.imwrite('model_results{}00008_analysis_V4_5_good{}ACM_with_kernels{}real_{}_predicted_{}_proba_{}%.png'.\n",
    "            format(slash, slash, slash, dict_sample_pred['real_class'], dict_sample_pred['prediction'], int(probs*100)), \n",
    "            all_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model structure image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from models.cnn_model import classifier_GD_2_ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out_features = 44\n",
    "parameter_config = {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':128, 'out_channel_2':128, 'kernel_size_1_W':14, 'kernel_size_2_W':14, 'max_pool_stride_1_W':6, 'max_pool_stride_2_W':6, 'n_epochs':30, 'learning_rate':1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_class = classifier_GD_2_ACM(n_out_features, parameter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 4, 300).requires_grad_(True)\n",
    "y = conv_class(x)\n",
    "make_dot(y, params=dict(list(conv_class.named_parameters()) + [('x', x)])).render(\"attached\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
