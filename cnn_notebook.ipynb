{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-functions\" data-toc-modified-id=\"Load-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load functions</a></span></li><li><span><a href=\"#Training-a-first-simple-NN\" data-toc-modified-id=\"Training-a-first-simple-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training a first simple NN</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-1D\" data-toc-modified-id=\"Looping-over-different-configuration-for-1D-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Looping over different configuration for 1D</a></span></li><li><span><a href=\"#Looping-over-different-configuration-for-2D\" data-toc-modified-id=\"Looping-over-different-configuration-for-2D-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Looping over different configuration for 2D</a></span></li><li><span><a href=\"#Looping-over-configurations-for-2D---ACM\" data-toc-modified-id=\"Looping-over-configurations-for-2D---ACM-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Looping over configurations for 2D - ACM</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_model import classifier_GD_1, classifier_GD_2\n",
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a first simple NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(selected_primer='V4', taxonomy_level=1)\n",
    "X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin='DairyDB', primers_origin='DairyDB', selected_primer='V4', taxonomy_level=1)\n",
    "n_out_features = len(dict_class_to_id)\n",
    "\n",
    "out_channel_1 = 30  # 10\n",
    "out_channel_2 = 30  # 20\n",
    "kernel_size_1_W = 7  # 7\n",
    "kernel_size_2_W = 7  # 7\n",
    "ratio_fc_1 = 1 / 2  # 1 / 2\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "conv_class = classifier_GD_2(n_out_features)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "_, _, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - 2D',\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in 4 dimensions (k=1)',\n",
    "                     sequence_origin='DairyDB',\n",
    "                     primers_origin='DairyDB',\n",
    "                     taxonomy_level=1,\n",
    "                     selected_primer='V4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class classifier_GD_1(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        super(classifier_GD_1, self).__init__()\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1 = kernel_size_1\n",
    "        self.max_pool_stride_1 = max_pool_stride_1\n",
    "        self.max_pool_stride_2 = max_pool_stride_2\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1 = self.kernel_size_1\n",
    "        self.kernel_size_2 = self.kernel_size_1\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1 = max_size - self.kernel_size_1 + 1\n",
    "        self.L_out_max_pool_1 = int((self.L_out_conv_1 - self.kernel_size_1) // self.max_pool_stride_1) + 1\n",
    "        self.L_out_conv_2 = self.L_out_max_pool_1 - self.kernel_size_2 + 1\n",
    "        self.L_out_max_pool_2 = int((self.L_out_conv_2 - self.kernel_size_2) // self.max_pool_stride_2) + 1\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2 * self.ratio_fc_1)\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=4 ** k_mer, out_channels=self.out_channel_1,\n",
    "                               kernel_size=self.kernel_size_1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=self.kernel_size_2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm1d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.out_channel_2 * self.L_out_max_pool_2,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_1, stride=self.max_pool_stride_1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool1d(x, kernel_size=self.kernel_size_2, stride=self.max_pool_stride_2)\n",
    "        x = x.view(-1, self.out_channel_2 * self.L_out_max_pool_2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1','max_pool_stride_1', 'max_pool_stride_2','ratio_fc_1','n_epochs','learning_rate','accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':50,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':100,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':150,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':200,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':250,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3},\n",
    "    {'sequence_origin':'DairyDB','primers_origin':'DairyDB','selected_primer':'V4','taxonomy_level':1,'dimension':1,'k_mer':1,'vector_max_size':300,'out_channel_1':32,'out_channel_2':64,'kernel_size_1':6,'max_pool_stride_1':2,'max_pool_stride_2':2,'ratio_fc_1':2/3,'n_epochs':50,'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1 = parameter_config['kernel_size_1']\n",
    "    max_pool_stride_1 = parameter_config['max_pool_stride_1']\n",
    "    max_pool_stride_2 = parameter_config['max_pool_stride_2']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_1(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over different configuration for 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_GD_2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        super(classifier_GD_2, self).__init__()\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1_W = kernel_size_1_W\n",
    "        self.kernel_size_2_W = kernel_size_2_W\n",
    "        self.ratio_fc_1 = ratio_fc_1\n",
    "        # FIXED PARAMETERS\n",
    "        self.kernel_size_1_H = 4 ** k_mer\n",
    "        self.padding_conv_1_H = 0\n",
    "        self.padding_conv_1_W = 0\n",
    "        self.kernel_size_max_pool_1_H = 1\n",
    "        self.max_pool_stride_1_H = 1\n",
    "        self.max_pool_stride_1_W = 8\n",
    "        self.kernel_size_2_H = 1\n",
    "        self.padding_conv_2_H = 0\n",
    "        self.padding_conv_2_W = 0\n",
    "        self.kernel_size_max_pool_2_H = 1\n",
    "        self.max_pool_stride_2_H = 1\n",
    "        self.max_pool_stride_2_W = 8\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1_W = self.kernel_size_1_W  # 7\n",
    "        self.kernel_size_max_pool_2_W = self.kernel_size_2_W  # 7\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_out_conv_1_H = 4 ** k_mer - self.kernel_size_1_H + 2 * self.padding_conv_1_H + 1  # 1\n",
    "        self.L_out_conv_1_W = max_size - self.kernel_size_1_W + 2 * self.padding_conv_1_W + 1  # 294\n",
    "        self.L_out_max_pool_1_H = int((self.L_out_conv_1_H - self.kernel_size_max_pool_1_H) // self.max_pool_stride_1_H) + 1  # 1\n",
    "        self.L_out_max_pool_1_W = int((self.L_out_conv_1_W - self.kernel_size_max_pool_1_W) // self.max_pool_stride_1_W) + 1  # 36\n",
    "        self.L_out_conv_2_H = self.L_out_max_pool_1_H - self.kernel_size_2_H + 2 * self.padding_conv_2_H + 1  # 1\n",
    "        self.L_out_conv_2_W = self.L_out_max_pool_1_W - self.kernel_size_2_W + 2 * self.padding_conv_2_W + 1  # 30\n",
    "        self.L_out_max_pool_2_H = int((self.L_out_conv_2_H - self.kernel_size_max_pool_2_H) // self.max_pool_stride_2_H) + 1  # 1\n",
    "        self.L_out_max_pool_2_W = int((self.L_out_conv_2_W - self.kernel_size_max_pool_2_W) // self.max_pool_stride_2_W) + 1  # 4\n",
    "        self.L_in_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W)  # 80\n",
    "        self.L_out_fc_1 = int(self.out_channel_2 * self.L_out_max_pool_2_H * self.L_out_max_pool_2_W * self.ratio_fc_1)  # 40\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.out_channel_1,\n",
    "                               kernel_size=(self.kernel_size_1_H, self.kernel_size_1_W),\n",
    "                               padding=(self.padding_conv_1_H, self.padding_conv_1_W))\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=(self.kernel_size_2_H, self.kernel_size_2_W),\n",
    "                               padding=(self.padding_conv_2_H, self.padding_conv_2_W))\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        # Hidden part\n",
    "        self.fc1 = nn.Linear(in_features=self.L_in_fc_1,\n",
    "                             out_features=self.L_out_fc_1)\n",
    "        self.ReLU3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=self.L_out_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 4 ** self.k_mer, self.max_size)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_1_H, self.kernel_size_max_pool_1_W),\n",
    "                         stride=(self.max_pool_stride_1_H, self.max_pool_stride_1_W))\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_2_H, self.kernel_size_max_pool_2_W),\n",
    "                         stride=(self.max_pool_stride_2_H, self.max_pool_stride_2_W))\n",
    "        x = x.view(-1, self.L_in_fc_1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x  # With CrossEntropyLoss directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','ratio_fc_1','n_epochs','learning_rate','accuracy', 'training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':32, 'out_channel_2':64, 'kernel_size_1_W':8, 'kernel_size_2_W':8, 'max_pool_stride_1_W':8, 'max_pool_stride_2_W':8, 'ratio_fc_1':1/2, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the following parameter configuration: \n",
      " {'sequence_origin': 'DairyDB', 'primers_origin': 'DairyDB', 'selected_primer': 'V4', 'taxonomy_level': 1, 'dimension': 2, 'k_mer': 1, 'vector_max_size': 300, 'out_channel_1': 32, 'out_channel_2': 64, 'kernel_size_1_W': 8, 'kernel_size_2_W': 8, 'max_pool_stride_1_W': 8, 'max_pool_stride_2_W': 8, 'ratio_fc_1': 0.5, 'n_epochs': 50, 'learning_rate': 0.001}\n",
      "Test - Loss: 0.0612 Acc: 0.0059\n",
      "Epoch 1 over 50\n",
      "Train - Loss: 0.0193 Acc: 0.7115\n",
      "Test - Loss: 0.0105 Acc: 0.8463\n",
      "Epoch 2 over 50\n",
      "Train - Loss: 0.0089 Acc: 0.8625\n",
      "Test - Loss: 0.0078 Acc: 0.8904\n",
      "Epoch 3 over 50\n",
      "Train - Loss: 0.0063 Acc: 0.9033\n",
      "Test - Loss: 0.0058 Acc: 0.9186\n",
      "Epoch 4 over 50\n",
      "Train - Loss: 0.0043 Acc: 0.9329\n",
      "Test - Loss: 0.0045 Acc: 0.9330\n",
      "Epoch 5 over 50\n",
      "Train - Loss: 0.0031 Acc: 0.9495\n",
      "Test - Loss: 0.0039 Acc: 0.9399\n",
      "Epoch 6 over 50\n",
      "Train - Loss: 0.0026 Acc: 0.9571\n",
      "Test - Loss: 0.0030 Acc: 0.9548\n",
      "Epoch 7 over 50\n",
      "Train - Loss: 0.0022 Acc: 0.9646\n",
      "Test - Loss: 0.0033 Acc: 0.9516\n",
      "Epoch 8 over 50\n",
      "Train - Loss: 0.0021 Acc: 0.9651\n",
      "Test - Loss: 0.0027 Acc: 0.9585\n",
      "Epoch 9 over 50\n",
      "Train - Loss: 0.0016 Acc: 0.9718\n",
      "Test - Loss: 0.0025 Acc: 0.9585\n",
      "Epoch 10 over 50\n",
      "Train - Loss: 0.0011 Acc: 0.9816\n",
      "Test - Loss: 0.0024 Acc: 0.9644\n",
      "Epoch 11 over 50\n",
      "Train - Loss: 0.0011 Acc: 0.9803\n",
      "Test - Loss: 0.0024 Acc: 0.9681\n",
      "Epoch 12 over 50\n",
      "Train - Loss: 0.0009 Acc: 0.9825\n",
      "Test - Loss: 0.0021 Acc: 0.9734\n",
      "Epoch 13 over 50\n",
      "Train - Loss: 0.0007 Acc: 0.9866\n",
      "Test - Loss: 0.0022 Acc: 0.9739\n",
      "Epoch 14 over 50\n",
      "Train - Loss: 0.0007 Acc: 0.9878\n",
      "Test - Loss: 0.0021 Acc: 0.9734\n",
      "Epoch 15 over 50\n",
      "Train - Loss: 0.0007 Acc: 0.9887\n",
      "Test - Loss: 0.0020 Acc: 0.9707\n",
      "Epoch 16 over 50\n",
      "Train - Loss: 0.0006 Acc: 0.9891\n",
      "Test - Loss: 0.0021 Acc: 0.9739\n",
      "Epoch 17 over 50\n",
      "Train - Loss: 0.0006 Acc: 0.9896\n",
      "Test - Loss: 0.0021 Acc: 0.9755\n",
      "Epoch 18 over 50\n",
      "Train - Loss: 0.0007 Acc: 0.9866\n",
      "Test - Loss: 0.0017 Acc: 0.9766\n",
      "Epoch 19 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9907\n",
      "Test - Loss: 0.0020 Acc: 0.9750\n",
      "Epoch 20 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9912\n",
      "Test - Loss: 0.0016 Acc: 0.9809\n",
      "Epoch 21 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9913\n",
      "Test - Loss: 0.0020 Acc: 0.9782\n",
      "Epoch 22 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9917\n",
      "Test - Loss: 0.0021 Acc: 0.9771\n",
      "Epoch 23 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9932\n",
      "Test - Loss: 0.0019 Acc: 0.9782\n",
      "Epoch 24 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9942\n",
      "Test - Loss: 0.0023 Acc: 0.9761\n",
      "Epoch 25 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9930\n",
      "Test - Loss: 0.0021 Acc: 0.9729\n",
      "Epoch 26 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9916\n",
      "Test - Loss: 0.0017 Acc: 0.9819\n",
      "Epoch 27 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9925\n",
      "Test - Loss: 0.0020 Acc: 0.9803\n",
      "Epoch 28 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9915\n",
      "Test - Loss: 0.0019 Acc: 0.9745\n",
      "Epoch 29 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9944\n",
      "Test - Loss: 0.0022 Acc: 0.9766\n",
      "Epoch 30 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9945\n",
      "Test - Loss: 0.0023 Acc: 0.9771\n",
      "Epoch 31 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9945\n",
      "Test - Loss: 0.0020 Acc: 0.9782\n",
      "Epoch 32 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9954\n",
      "Test - Loss: 0.0027 Acc: 0.9745\n",
      "Epoch 33 over 50\n",
      "Train - Loss: 0.0007 Acc: 0.9867\n",
      "Test - Loss: 0.0018 Acc: 0.9798\n",
      "Epoch 34 over 50\n",
      "Train - Loss: 0.0004 Acc: 0.9904\n",
      "Test - Loss: 0.0021 Acc: 0.9787\n",
      "Epoch 35 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9967\n",
      "Test - Loss: 0.0024 Acc: 0.9809\n",
      "Epoch 36 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9938\n",
      "Test - Loss: 0.0019 Acc: 0.9793\n",
      "Epoch 37 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9940\n",
      "Test - Loss: 0.0021 Acc: 0.9830\n",
      "Epoch 38 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9961\n",
      "Test - Loss: 0.0026 Acc: 0.9750\n",
      "Epoch 39 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9963\n",
      "Test - Loss: 0.0024 Acc: 0.9782\n",
      "Epoch 40 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9953\n",
      "Test - Loss: 0.0029 Acc: 0.9771\n",
      "Epoch 41 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9953\n",
      "Test - Loss: 0.0030 Acc: 0.9750\n",
      "Epoch 42 over 50\n",
      "Train - Loss: 0.0003 Acc: 0.9934\n",
      "Test - Loss: 0.0019 Acc: 0.9819\n",
      "Epoch 43 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9953\n",
      "Test - Loss: 0.0018 Acc: 0.9814\n",
      "Epoch 44 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9963\n",
      "Test - Loss: 0.0023 Acc: 0.9793\n",
      "Epoch 45 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9955\n",
      "Test - Loss: 0.0017 Acc: 0.9846\n",
      "Epoch 46 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9958\n",
      "Test - Loss: 0.0020 Acc: 0.9835\n",
      "Epoch 47 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9953\n",
      "Test - Loss: 0.0030 Acc: 0.9798\n",
      "Epoch 48 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9954\n",
      "Test - Loss: 0.0025 Acc: 0.9793\n",
      "Epoch 49 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9953\n",
      "Test - Loss: 0.0021 Acc: 0.9771\n",
      "Epoch 50 over 50\n",
      "Train - Loss: 0.0002 Acc: 0.9955\n",
      "Test - Loss: 0.0021 Acc: 0.9824\n",
      "Test - Loss: 0.0023 Acc: 0.9814\n"
     ]
    }
   ],
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1_W = parameter_config['kernel_size_1_W']\n",
    "    kernel_size_2_W = parameter_config['kernel_size_2_W']\n",
    "    ratio_fc_1 = parameter_config['ratio_fc_1']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "    \n",
    "    \n",
    "\n",
    "    conv_class = classifier_GD_2(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over configurations for 2D - ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_preprocessing import main_preprocessing_cnn\n",
    "from models.cnn_train_test import train, test\n",
    "from models.cnn_model_statistics import main_cnn_stats_model\n",
    "from models.loading_model_data import main_loading_model_data\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import save_update_cnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_module(nn.Module):\n",
    "    def __init__(self, k_mer: int = 1, max_size: int = 300, out_channel_2:int = 60):\n",
    "        super(conv_module, self).__init__()\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        # PARAMETERS\n",
    "        self.out_channel_1 = out_channel_1\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.kernel_size_1_W = kernel_size_1_W\n",
    "        self.kernel_size_2_W = kernel_size_2_W\n",
    "        # FIXED PARAMETERS\n",
    "        self.kernel_size_1_H = 4 ** k_mer\n",
    "        self.padding_conv_1_H = 0\n",
    "        self.padding_conv_1_W = 0\n",
    "        self.kernel_size_max_pool_1_H = 1\n",
    "        self.max_pool_stride_1_H = 1\n",
    "        self.max_pool_stride_1_W = max_pool_stride_1_W\n",
    "        self.kernel_size_2_H = 1\n",
    "        self.padding_conv_2_H = 0\n",
    "        self.padding_conv_2_W = 0\n",
    "        self.kernel_size_max_pool_2_H = 1\n",
    "        self.max_pool_stride_2_H = 1\n",
    "        self.max_pool_stride_2_W = max_pool_stride_2_W\n",
    "        # COPIED PARAMETERS\n",
    "        self.kernel_size_max_pool_1_W = self.kernel_size_1_W\n",
    "        self.kernel_size_max_pool_2_W = self.kernel_size_2_W\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.out_channel_1,\n",
    "                               kernel_size=(self.kernel_size_1_H, self.kernel_size_1_W),\n",
    "                               padding=(self.padding_conv_1_H, self.padding_conv_1_W))\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_channel_1)\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        # Layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_channel_1, out_channels=self.out_channel_2,\n",
    "                               kernel_size=(self.kernel_size_2_H, self.kernel_size_2_W),\n",
    "                               padding=(self.padding_conv_2_H, self.padding_conv_2_W))\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channel_2)\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 4 ** self.k_mer, self.max_size)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.ReLU1(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_1_H, self.kernel_size_max_pool_1_W),\n",
    "                         stride=(self.max_pool_stride_1_H, self.max_pool_stride_1_W))\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = F.max_pool2d(x,\n",
    "                         kernel_size=(self.kernel_size_max_pool_2_H, self.kernel_size_max_pool_2_W),\n",
    "                         stride=(self.max_pool_stride_2_H, self.max_pool_stride_2_W))\n",
    "        return x\n",
    "\n",
    "\n",
    "class fc_module(nn.Module):\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300, out_channel_2:int = 60):\n",
    "        super(fc_module, self).__init__()\n",
    "        self.k_mer = k_mer\n",
    "        self.max_size = max_size\n",
    "        # PARAMETERS\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        # SIZE COMPUTATION\n",
    "        self.L_in_fc_1 = self.out_channel_2\n",
    "        # FC part\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc1 = nn.Linear(in_features=self.L_in_fc_1,\n",
    "                             out_features=n_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(-1, self.L_in_fc_1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class classifier_GD_2_ACM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out_features: int, k_mer: int = 1, max_size: int = 300):\n",
    "        super(classifier_GD_2_ACM, self).__init__()\n",
    "        self.out_channel_2 = out_channel_2\n",
    "        self.conv = conv_module(k_mer=k_mer, max_size=max_size, out_channel_2=self.out_channel_2)\n",
    "        self.fully_connected = fc_module(n_out_features=n_out_features, k_mer=k_mer, max_size=max_size,\n",
    "                                         out_channel_2=self.out_channel_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fully_connected(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['sequence_origin','primers_origin','selected_primer','taxonomy_level','dimension','k_mer','vector_max_size','out_channel_1','out_channel_2','kernel_size_1_W','kernel_size_2_W','max_pool_stride_1_W','max_pool_stride_2_W','n_epochs','learning_rate','accuracy','training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_config_list = [\n",
    "    {'sequence_origin':'DairyDB', 'primers_origin':'DairyDB', 'selected_primer':'V4', 'taxonomy_level':1, 'dimension':2, 'k_mer':1, 'vector_max_size':300, 'out_channel_1':32, 'out_channel_2':64, 'kernel_size_1_W':8, 'kernel_size_2_W':8, 'max_pool_stride_1_W':8, 'max_pool_stride_2_W':8, 'n_epochs':50, 'learning_rate':1e-3}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter_config in parameter_config_list:\n",
    "    dim = parameter_config['dimension']\n",
    "    k_mer = parameter_config['k_mer']\n",
    "    max_size = parameter_config['vector_max_size']\n",
    "    file_path = 'results/models/CNN {}D - ACM - CNN({}) - accuracies.csv'.format(\n",
    "        dim, k_mer\n",
    "    )\n",
    "    print('Working with the following parameter configuration: \\n {}'.format(parameter_config))\n",
    "    train_loader, test_loader, dict_class_to_id, dict_id_to_class = main_preprocessing_cnn(\n",
    "        sequence_origin=parameter_config['sequence_origin'], \n",
    "        primers_origin=parameter_config['primers_origin'],\n",
    "        selected_primer=parameter_config['selected_primer'], \n",
    "        taxonomy_level=parameter_config['taxonomy_level'],\n",
    "        max_size=max_size,\n",
    "        k_mer=k_mer\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = main_loading_model_data(sequence_origin=parameter_config['sequence_origin'], \n",
    "                                                               primers_origin=parameter_config['primers_origin'], \n",
    "                                                               selected_primer=parameter_config['selected_primer'], \n",
    "                                                               taxonomy_level=parameter_config['taxonomy_level'])\n",
    "    n_out_features = len(dict_class_to_id)\n",
    "\n",
    "    out_channel_1 = parameter_config['out_channel_1']\n",
    "    out_channel_2 = parameter_config['out_channel_2']\n",
    "    kernel_size_1_W = parameter_config['kernel_size_1_W']\n",
    "    kernel_size_2_W = parameter_config['kernel_size_2_W']\n",
    "    max_pool_stride_1_W = parameter_config['max_pool_stride_1_W']\n",
    "    max_pool_stride_2_W = parameter_config['max_pool_stride_2_W']\n",
    "\n",
    "    n_epochs = parameter_config['n_epochs']\n",
    "\n",
    "    conv_class = classifier_GD_2_ACM(n_out_features, k_mer, max_size)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    learning_rate = parameter_config['learning_rate']\n",
    "    optimizer_cl = torch.optim.Adam(conv_class.parameters(), lr=learning_rate)\n",
    "\n",
    "    _, _, _, _ = test(conv_class, test_loader, loss_fn)\n",
    "    begin_time = time.time()\n",
    "    loss_train, acc_train, loss_test, acc_test = train(conv_class, train_loader, test_loader, loss_fn, optimizer_cl, n_epochs=n_epochs)\n",
    "    end_time = time.time()\n",
    "    final_test_loss, accuracy, y_test_torch, y_pred_torch = test(conv_class, test_loader, loss_fn)\n",
    "    \n",
    "    parameter_config['accuracy'] = accuracy\n",
    "    parameter_config['training_time'] = end_time - begin_time\n",
    "    \n",
    "    main_cnn_stats_model(y_train, y_test_torch, y_pred_torch, dict_id_to_class, loss_train, loss_test, acc_train, acc_test,\n",
    "                     make_plot=True,\n",
    "                     model_name='CNN - Aoki - {}D'.format(dim),\n",
    "                     model_class=conv_class,\n",
    "                     model_preprocessing='OHE of letters in {} dimensions (k={}) - max size = {}'.format(4**k_mer, k_mer, max_size),\n",
    "                     sequence_origin=parameter_config['sequence_origin'],\n",
    "                     primers_origin=parameter_config['primers_origin'],\n",
    "                     taxonomy_level=parameter_config['taxonomy_level'],\n",
    "                     selected_primer=parameter_config['selected_primer'])\n",
    "    \n",
    "    save_update_cnn(file_path, parameter_config.keys(), parameter_config.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
